{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enchant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:45.526377800Z",
     "start_time": "2024-01-14T20:20:44.333216200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "file_path = 'Dataset/Spelling Dataset/spell-errors.txt'\n",
    "\n",
    "dictionary_file_path = 'Dataset/Spelling Dataset/test/Dictionary/dictionary.data'\n",
    "\n",
    "del_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/del-confusion.data'\n",
    "ins_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/ins-confusion.data'\n",
    "sub_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/sub-confusion.data'\n",
    "tra_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/Transposition-confusion.data'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:46.212552800Z",
     "start_time": "2024-01-14T20:20:46.196912Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words in the dictionary:\n",
      "['aa' 'aah' 'aahed' 'aahing' 'aahs' 'aal' 'aalii' 'aaliis' 'aals'\n",
      " 'aardvark']\n"
     ]
    }
   ],
   "source": [
    "def read_dictionary(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        dictionary = np.array([word.strip() for word in file])\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "dictionary = read_dictionary(dictionary_file_path)\n",
    "\n",
    "print(\"First 10 words in the dictionary:\")\n",
    "print(dictionary[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:46.787942800Z",
     "start_time": "2024-01-14T20:20:46.701062900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "        CorrectWord                                 Typos\n0           raining                    [rainning, raning]\n1          writings                           [writtings]\n2     disparagingly                         [disparingly]\n3            yellow                               [yello]\n4              four  [forer, fours, fuore, fore*5, for*4]\n...             ...                                   ...\n7836          jewel                          [jewl, jule]\n7837   commencement                         [commencment]\n7838    suppressing                          [supressing]\n7839         tonner                               [toner]\n7840           sash                                [sach]\n\n[7841 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CorrectWord</th>\n      <th>Typos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>raining</td>\n      <td>[rainning, raning]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>writings</td>\n      <td>[writtings]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disparagingly</td>\n      <td>[disparingly]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>yellow</td>\n      <td>[yello]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>four</td>\n      <td>[forer, fours, fuore, fore*5, for*4]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7836</th>\n      <td>jewel</td>\n      <td>[jewl, jule]</td>\n    </tr>\n    <tr>\n      <th>7837</th>\n      <td>commencement</td>\n      <td>[commencment]</td>\n    </tr>\n    <tr>\n      <th>7838</th>\n      <td>suppressing</td>\n      <td>[supressing]</td>\n    </tr>\n    <tr>\n      <th>7839</th>\n      <td>tonner</td>\n      <td>[toner]</td>\n    </tr>\n    <tr>\n      <th>7840</th>\n      <td>sash</td>\n      <td>[sach]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7841 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_typo_data(file_path):\n",
    "    typo_data = {'CorrectWord': [], 'Typos': []}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) == 2:\n",
    "                correct_word, typos = parts[0], parts[1].split(',')\n",
    "                typo_data['CorrectWord'].append(correct_word)\n",
    "                typo_data['Typos'].append([typo.strip() for typo in typos])\n",
    "\n",
    "    return pd.DataFrame(typo_data)\n",
    "\n",
    "typo_df = load_typo_data(file_path)\n",
    "typo_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:47.409176900Z",
     "start_time": "2024-01-14T20:20:47.345871700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def read_confusion_matrix(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        confusion_matrix_str = file.read()\n",
    "        confusion_matrix = ast.literal_eval(confusion_matrix_str)\n",
    "\n",
    "    return confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:48.043004400Z",
     "start_time": "2024-01-14T20:20:48.021982500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "del_confusion_matrix = read_confusion_matrix(del_confusion_matrix_file_path)\n",
    "ins_confusion_matrix = read_confusion_matrix(ins_confusion_matrix_file_path)\n",
    "sub_confusion_matrix = read_confusion_matrix(sub_confusion_matrix_file_path)\n",
    "tra_confusion_matrix = read_confusion_matrix(tra_confusion_matrix_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:48.737063Z",
     "start_time": "2024-01-14T20:20:48.705808400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damerau–Levenshtein distance between 'acress' and 'access': 1\n"
     ]
    }
   ],
   "source": [
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the Damerau–Levenshtein distance between two strings.\n",
    "    \"\"\"\n",
    "    len_s1 = len(s1)\n",
    "    len_s2 = len(s2)\n",
    "    d = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
    "\n",
    "    for i in range(len_s1 + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len_s2 + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i - 1][j] + 1,  # deletion\n",
    "                d[i][j - 1] + 1,  # insertion\n",
    "                d[i - 1][j - 1] + cost,  # substitution\n",
    "            )\n",
    "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  # transposition\n",
    "\n",
    "    return d[len_s1][len_s2]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "s1 = \"acress\"\n",
    "s2 = \"access\"\n",
    "\n",
    "distance = damerau_levenshtein_distance(s1, s2)\n",
    "print(f\"Damerau–Levenshtein distance between '{s1}' and '{s2}': {distance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:49.516206100Z",
     "start_time": "2024-01-14T20:20:49.510209800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['problems']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_correct_words(typo):\n",
    "    # correct_words = typo_df.loc[typo_df['Typos'].apply(lambda x: typo in x), 'CorrectWord'].tolist()\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    correct_words = d.suggest(typo)\n",
    "    correct_words2 = []\n",
    "    for candida in correct_words:\n",
    "        if damerau_levenshtein_distance(typo, candida) == 1:\n",
    "            correct_words2.append(candida)\n",
    "    return correct_words2\n",
    "\n",
    "typo = 'problem'\n",
    "correct_words = find_correct_words(typo)\n",
    "correct_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:50.419252900Z",
     "start_time": "2024-01-14T20:20:50.246458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "with open('Dataset/Spelling Dataset/test/Dictionary/Dataset.data', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Split the text into sentences or paragraphs based on your dataset structure\n",
    "sentences = text_data.split('<s>')\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "tokenized_sentences = np.concatenate(tokenized_sentences, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:20:57.574031100Z",
     "start_time": "2024-01-14T20:20:54.410229300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "             word   count\n72392         the  282764\n5941          and  155164\n51147          of  151505\n73254          to  115135\n37328          in   87003\n...           ...     ...\n39687  jaculation       1\n39690       jadau       1\n39694       jadon       1\n39695      jaeger       1\n81342        zzzz       1\n\n[81343 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>72392</th>\n      <td>the</td>\n      <td>282764</td>\n    </tr>\n    <tr>\n      <th>5941</th>\n      <td>and</td>\n      <td>155164</td>\n    </tr>\n    <tr>\n      <th>51147</th>\n      <td>of</td>\n      <td>151505</td>\n    </tr>\n    <tr>\n      <th>73254</th>\n      <td>to</td>\n      <td>115135</td>\n    </tr>\n    <tr>\n      <th>37328</th>\n      <td>in</td>\n      <td>87003</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39687</th>\n      <td>jaculation</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39690</th>\n      <td>jadau</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39694</th>\n      <td>jadon</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39695</th>\n      <td>jaeger</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>81342</th>\n      <td>zzzz</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>81343 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, counts = np.unique(tokenized_sentences, return_counts=True)\n",
    "dataset = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'count': counts\n",
    "}).sort_values(['count'], ascending=False)\n",
    "# dataset.map(lambda x : apply())\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:21:26.907959Z",
     "start_time": "2024-01-14T20:21:23.916125700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "({'gw': 1,\n  'gv': 0,\n  'gu': 0,\n  'gt': 21,\n  'gs': 13,\n  'gr': 5,\n  'gq': 3,\n  'gp': 1,\n  'gz': 0,\n  'gy': 3,\n  'gx': 0,\n  'gg': 0,\n  'gf': 2,\n  'ge': 9,\n  'gd': 11,\n  'gc': 11,\n  'gb': 1,\n  'ga': 4,\n  'go': 2,\n  'gn': 0,\n  'gm': 0,\n  'gl': 3,\n  'gk': 1,\n  'gj': 1,\n  'gi': 0,\n  'gh': 0,\n  'tz': 6,\n  'tx': 0,\n  'ty': 7,\n  'tv': 2,\n  'tw': 19,\n  'tt': 0,\n  'tu': 0,\n  'tr': 11,\n  'ts': 37,\n  'tp': 6,\n  'tq': 0,\n  'tn': 5,\n  'to': 5,\n  'tl': 14,\n  'tm': 9,\n  'tj': 1,\n  'tk': 0,\n  'th': 5,\n  'ti': 0,\n  'tf': 5,\n  'tg': 19,\n  'td': 42,\n  'te': 7,\n  'tb': 4,\n  'tc': 9,\n  'ta': 3,\n  'vu': 0,\n  'zl': 7,\n  'zm': 5,\n  'zn': 0,\n  'zo': 0,\n  'zh': 0,\n  'zi': 0,\n  'zj': 0,\n  'zk': 0,\n  'zd': 7,\n  'ze': 0,\n  'zf': 0,\n  'zg': 0,\n  'za': 0,\n  'zb': 0,\n  'zc': 0,\n  'zx': 0,\n  'zy': 3,\n  'zz': 0,\n  'zt': 3,\n  'zu': 0,\n  'zv': 0,\n  'zw': 0,\n  'zp': 0,\n  'zq': 0,\n  'zr': 2,\n  'zs': 21,\n  'wl': 0,\n  'va': 0,\n  'vc': 7,\n  'wk': 1,\n  'vh': 0,\n  'wj': 0,\n  'vi': 0,\n  'vj': 0,\n  'vk': 0,\n  'vl': 1,\n  'vm': 0,\n  'wi': 0,\n  'vn': 0,\n  'vo': 1,\n  'me': 0,\n  'md': 8,\n  'mg': 0,\n  'mf': 2,\n  'ma': 1,\n  'mc': 7,\n  'mb': 3,\n  'mm': 0,\n  'ml': 4,\n  'mo': 0,\n  'mn': 180,\n  'mi': 0,\n  'mh': 6,\n  'mk': 4,\n  'mj': 0,\n  'mu': 13,\n  'mt': 15,\n  'mw': 2,\n  'mv': 3,\n  'mq': 0,\n  'mp': 6,\n  'ms': 9,\n  'mr': 0,\n  'vt': 3,\n  'my': 3,\n  'mx': 2,\n  'mz': 0,\n  'vv': 0,\n  'vw': 0,\n  'vx': 0,\n  'vz': 0,\n  'fp': 0,\n  'fq': 0,\n  'fr': 6,\n  'fs': 4,\n  'ft': 12,\n  'fu': 0,\n  'fv': 0,\n  'fw': 2,\n  'fx': 0,\n  'fy': 0,\n  'fz': 0,\n  'fa': 0,\n  'fb': 15,\n  'fc': 0,\n  'fd': 3,\n  'fe': 1,\n  'ff': 0,\n  'fg': 5,\n  'fh': 2,\n  'fi': 0,\n  'fj': 0,\n  'fk': 0,\n  'fl': 3,\n  'fm': 4,\n  'fn': 1,\n  'fo': 0,\n  'sz': 1,\n  'sy': 20,\n  'sx': 3,\n  'ss': 0,\n  'sr': 14,\n  'sq': 0,\n  'sp': 7,\n  'sw': 5,\n  'sv': 0,\n  'su': 0,\n  'st': 15,\n  'sk': 0,\n  'sj': 1,\n  'si': 0,\n  'sh': 1,\n  'so': 1,\n  'sn': 6,\n  'sm': 0,\n  'sl': 27,\n  'sc': 27,\n  'sb': 8,\n  'sa': 11,\n  'sg': 0,\n  'sf': 4,\n  'se': 35,\n  'sd': 33,\n  'lf': 4,\n  'lg': 5,\n  'ld': 4,\n  'le': 0,\n  'lb': 10,\n  'lc': 1,\n  'la': 2,\n  'ln': 14,\n  'lo': 2,\n  'll': 0,\n  'lm': 0,\n  'lj': 0,\n  'lk': 1,\n  'lh': 6,\n  'li': 13,\n  'lv': 0,\n  'lw': 0,\n  'lt': 2,\n  'lu': 0,\n  'lr': 11,\n  'ls': 10,\n  'lp': 5,\n  'lq': 0,\n  'lz': 0,\n  'lx': 0,\n  'ly': 0,\n  'wq': 0,\n  'yh': 7,\n  'yk': 0,\n  'yj': 0,\n  'ym': 2,\n  'yl': 0,\n  'yo': 6,\n  'yn': 0,\n  'ya': 0,\n  'yc': 2,\n  'yb': 0,\n  'ye': 15,\n  'yd': 0,\n  'yg': 1,\n  'yf': 0,\n  'yy': 0,\n  'yx': 1,\n  'yz': 0,\n  'yq': 0,\n  'yp': 1,\n  'ys': 36,\n  'yr': 7,\n  'yu': 5,\n  'yt': 8,\n  'yw': 0,\n  'yv': 0,\n  'em': 0,\n  'el': 3,\n  'eo': 93,\n  'en': 5,\n  'ei': 89,\n  'eh': 0,\n  'ek': 0,\n  'ej': 0,\n  'ee': 0,\n  'ed': 11,\n  'eg': 2,\n  'ef': 2,\n  'ea': 388,\n  'ec': 3,\n  'eb': 0,\n  'ey': 18,\n  'ex': 0,\n  'ez': 0,\n  'eu': 15,\n  'et': 6,\n  'ew': 1,\n  'ev': 0,\n  'eq': 0,\n  'ep': 0,\n  'es': 12,\n  'er': 14,\n  'rt': 22,\n  'ru': 4,\n  'rv': 0,\n  'rw': 0,\n  'rp': 14,\n  'rq': 0,\n  'rr': 0,\n  'rs': 12,\n  'rx': 1,\n  'ry': 0,\n  'rz': 0,\n  'rd': 30,\n  're': 12,\n  'rf': 2,\n  'rg': 2,\n  'ra': 0,\n  'rb': 14,\n  'rc': 0,\n  'rl': 8,\n  'rm': 4,\n  'rn': 20,\n  'ro': 1,\n  'rh': 8,\n  'ri': 2,\n  'rj': 0,\n  'rk': 5,\n  'xj': 0,\n  'xk': 0,\n  'xh': 0,\n  'xi': 0,\n  'xn': 0,\n  'xo': 0,\n  'xl': 0,\n  'xm': 0,\n  'xb': 0,\n  'xc': 0,\n  'xa': 0,\n  'xf': 0,\n  'xg': 0,\n  'xd': 2,\n  'xe': 0,\n  'xz': 0,\n  'xx': 0,\n  'xy': 0,\n  'xr': 0,\n  'xs': 9,\n  'xp': 0,\n  'xq': 0,\n  'xv': 0,\n  'xw': 0,\n  'xt': 0,\n  'xu': 0,\n  'wy': 0,\n  'wx': 0,\n  'kc': 8,\n  'kb': 2,\n  'ka': 1,\n  'kg': 2,\n  'kf': 1,\n  'ke': 1,\n  'kd': 4,\n  'kk': 0,\n  'kj': 0,\n  'ki': 0,\n  'kh': 5,\n  'ko': 2,\n  'kn': 0,\n  'km': 5,\n  'kl': 0,\n  'ks': 6,\n  'kr': 0,\n  'kq': 0,\n  'kp': 0,\n  'kw': 4,\n  'kv': 0,\n  'ku': 0,\n  'kt': 0,\n  'kz': 3,\n  'ky': 0,\n  'kx': 0,\n  'dn': 3,\n  'do': 0,\n  'dl': 3,\n  'dm': 7,\n  'dj': 0,\n  'dk': 2,\n  'dh': 5,\n  'di': 0,\n  'df': 0,\n  'dg': 5,\n  'dd': 0,\n  'de': 12,\n  'db': 10,\n  'dc': 13,\n  'da': 1,\n  'dz': 0,\n  'dx': 0,\n  'dy': 2,\n  'dv': 0,\n  'dw': 4,\n  'dt': 22,\n  'du': 0,\n  'dr': 43,\n  'ds': 30,\n  'dp': 1,\n  'dq': 0,\n  'qq': 0,\n  'qp': 0,\n  'qs': 0,\n  'qr': 0,\n  'qu': 0,\n  'qt': 0,\n  'qw': 0,\n  'qv': 0,\n  'qy': 0,\n  'qx': 0,\n  'qz': 0,\n  'qa': 0,\n  'qc': 1,\n  'qb': 0,\n  'qe': 0,\n  'qd': 0,\n  'qg': 27,\n  'qf': 0,\n  'qi': 0,\n  'qh': 0,\n  'qk': 0,\n  'qj': 0,\n  'qm': 0,\n  'ql': 0,\n  'qo': 0,\n  'qn': 0,\n  'wc': 1,\n  'wb': 2,\n  'wa': 2,\n  'wo': 0,\n  'wn': 0,\n  'wm': 0,\n  'wg': 0,\n  'wf': 0,\n  'we': 1,\n  'wd': 0,\n  'jx': 0,\n  'jy': 0,\n  'jz': 0,\n  'jt': 0,\n  'ju': 0,\n  'jv': 0,\n  'jw': 0,\n  'jp': 0,\n  'jq': 0,\n  'jr': 0,\n  'js': 5,\n  'jl': 2,\n  'jm': 1,\n  'jn': 0,\n  'jo': 0,\n  'jh': 0,\n  'ji': 0,\n  'jj': 0,\n  'jk': 0,\n  'jd': 9,\n  'je': 0,\n  'jf': 0,\n  'jg': 1,\n  'ja': 0,\n  'jb': 1,\n  'jc': 1,\n  'ww': 0,\n  'wv': 0,\n  'wu': 1,\n  'wt': 3,\n  'ws': 3,\n  'wr': 6,\n  'ck': 1,\n  'cj': 0,\n  'ci': 0,\n  'ch': 0,\n  'co': 1,\n  'cn': 9,\n  'cm': 7,\n  'cl': 0,\n  'cc': 0,\n  'cb': 5,\n  'ca': 6,\n  'wp': 7,\n  'cg': 5,\n  'cf': 9,\n  'ce': 0,\n  'cd': 16,\n  'cz': 0,\n  'cy': 1,\n  'cx': 1,\n  'cs': 39,\n  'cr': 5,\n  'cq': 2,\n  'cp': 10,\n  'cw': 7,\n  'cv': 3,\n  'cu': 1,\n  'ct': 40,\n  'pr': 1,\n  'ps': 3,\n  'pp': 0,\n  'pq': 0,\n  'pv': 4,\n  'pw': 1,\n  'pt': 6,\n  'pu': 0,\n  'pz': 0,\n  'px': 0,\n  'py': 0,\n  'wz': 0,\n  'pb': 11,\n  'pc': 1,\n  'pa': 0,\n  'pf': 6,\n  'pg': 5,\n  'pd': 2,\n  'pe': 0,\n  'pj': 9,\n  'pk': 0,\n  'ph': 0,\n  'pi': 2,\n  'pn': 6,\n  'po': 15,\n  'pl': 2,\n  'pm': 7,\n  'iy': 15,\n  'ix': 1,\n  'vb': 0,\n  'iz': 0,\n  'vd': 0,\n  've': 0,\n  'vf': 3,\n  'vg': 0,\n  'iq': 0,\n  'ip': 0,\n  'is': 2,\n  'ir': 0,\n  'iu': 47,\n  'it': 1,\n  'iw': 2,\n  'iv': 0,\n  'ii': 0,\n  'ih': 0,\n  'ik': 0,\n  'ij': 0,\n  'im': 0,\n  'il': 6,\n  'io': 49,\n  'in': 0,\n  'ia': 103,\n  'vy': 0,\n  'ic': 0,\n  'ib': 0,\n  'ie': 146,\n  'id': 0,\n  'ig': 1,\n  'if': 0,\n  'wh': 2,\n  'yi': 15,\n  'vr': 0,\n  'vs': 8,\n  'bd': 9,\n  'be': 2,\n  'bf': 2,\n  'bg': 3,\n  'ba': 0,\n  'bb': 0,\n  'bc': 9,\n  'bl': 5,\n  'bm': 11,\n  'bn': 5,\n  'bo': 0,\n  'bh': 1,\n  'bi': 0,\n  'bj': 0,\n  'bk': 0,\n  'bt': 1,\n  'bu': 0,\n  'bv': 0,\n  'bw': 8,\n  'bp': 10,\n  'bq': 0,\n  'br': 0,\n  'bs': 2,\n  'bx': 0,\n  'by': 0,\n  'bz': 0,\n  'oo': 0,\n  'on': 0,\n  'om': 0,\n  'ol': 0,\n  'ok': 2,\n  'oj': 0,\n  'oi': 25,\n  'oh': 0,\n  'og': 0,\n  'of': 0,\n  'oe': 116,\n  'od': 3,\n  'oc': 1,\n  'ob': 1,\n  'oa': 91,\n  'oz': 0,\n  'oy': 18,\n  'ox': 0,\n  'ow': 0,\n  'ov': 0,\n  'ou': 39,\n  'ot': 14,\n  'os': 4,\n  'or': 2,\n  'oq': 0,\n  'op': 14,\n  'hz': 0,\n  'hx': 0,\n  'hy': 0,\n  'hr': 3,\n  'hs': 1,\n  'hp': 3,\n  'hq': 0,\n  'hv': 0,\n  'hw': 2,\n  'ht': 11,\n  'hu': 0,\n  'hj': 0,\n  'hk': 2,\n  'hh': 0,\n  'hi': 0,\n  'hn': 14,\n  'ho': 2,\n  'hl': 0,\n  'hm': 12,\n  'hb': 8,\n  'hc': 0,\n  'ha': 1,\n  'hf': 0,\n  'hg': 0,\n  'hd': 3,\n  'he': 0,\n  'uy': 8,\n  'ux': 0,\n  'uz': 0,\n  'uu': 0,\n  'ut': 0,\n  'uw': 2,\n  'uv': 0,\n  'uq': 0,\n  'up': 0,\n  'us': 0,\n  'ur': 4,\n  'um': 0,\n  'ul': 0,\n  'uo': 43,\n  'un': 2,\n  'ui': 64,\n  'uh': 0,\n  'uk': 0,\n  'uj': 0,\n  'ue': 44,\n  'ud': 0,\n  'ug': 0,\n  'uf': 0,\n  'ua': 20,\n  'uc': 0,\n  'ub': 0,\n  'aa': 0,\n  'ac': 7,\n  'ab': 0,\n  'ae': 342,\n  'ad': 1,\n  'ag': 0,\n  'af': 0,\n  'ai': 118,\n  'ah': 2,\n  'ak': 1,\n  'aj': 0,\n  'am': 0,\n  'al': 0,\n  'ao': 76,\n  'an': 3,\n  'aq': 0,\n  'ap': 0,\n  'as': 35,\n  'ar': 1,\n  'au': 9,\n  'at': 9,\n  'aw': 1,\n  'av': 0,\n  'ay': 5,\n  'ax': 0,\n  'az': 0,\n  'nh': 19,\n  'ni': 1,\n  'nj': 0,\n  'nk': 4,\n  'nl': 35,\n  'nm': 78,\n  'nn': 0,\n  'no': 0,\n  'na': 2,\n  'nb': 7,\n  'nc': 6,\n  'nd': 5,\n  'ne': 3,\n  'nf': 0,\n  'ng': 1,\n  'nx': 2,\n  'ny': 0,\n  'nz': 2,\n  'np': 7,\n  'nq': 0,\n  'nr': 28,\n  'ns': 5,\n  'nt': 7,\n  'nu': 0,\n  'nv': 0,\n  'nw': 1,\n  'vp': 0,\n  'vq': 0},\n \"'s\")"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_type_and_letter(x, w):\n",
    "    if len(x) > len(w):\n",
    "        for i in range(len(x)):\n",
    "            if i == len(w) - 1 or w[i] != x[i]:\n",
    "                return ins_confusion_matrix, x[i]\n",
    "            else:\n",
    "                continue\n",
    "    elif len(x) < len(w):\n",
    "        for i in range(len(x)):\n",
    "            if w[i] == x[i]:\n",
    "                continue\n",
    "            else:\n",
    "                return del_confusion_matrix, w[i]\n",
    "    else: #transposition or substitution\n",
    "        for i in range(len(x)):\n",
    "            if w[i] == x[i]:\n",
    "                continue\n",
    "            else:\n",
    "                if i == len(x)-1:\n",
    "                    return sub_confusion_matrix, w[i]+x[i]\n",
    "                else:\n",
    "                    if w[i+1] == x[i+1]:\n",
    "                        return sub_confusion_matrix, w[i]+x[i]\n",
    "                    elif w[i] == x[i+1]:\n",
    "                        return tra_confusion_matrix, w[i]+w[i+1]\n",
    "                    else:\n",
    "                        return \"?\"\n",
    "candidates = ['caress', 'acres', 'cress', 'actress', 'across', 'access', \"acre's\", 'a cress', 'acre ss', 'acre-ss', 'acres s']\n",
    "find_type_and_letter(\"acress\", \"acre's\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:21:49.557312100Z",
     "start_time": "2024-01-14T20:21:49.519830400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def Pw(w):\n",
    "    if not dataset.loc[dataset['word'] == w].empty :\n",
    "        frequency_of_w = dataset.loc[dataset['word'] == w].iloc[0]['count']\n",
    "    else:\n",
    "        frequency_of_w = 0\n",
    "    count_of_all_tokens = dataset['count'].sum()\n",
    "    size_of_vocab = len(dataset['word'])\n",
    "    return (frequency_of_w + 1) / (count_of_all_tokens + size_of_vocab)\n",
    "\n",
    "def Pxw(x, w):\n",
    "    confusion_matrix, characters = find_type_and_letter(x, w)\n",
    "    print()\n",
    "    soorat = confusion_matrix[characters] + 1\n",
    "    makhraj = text_data.count(characters) + 1\n",
    "    return soorat / makhraj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:23:24.551675700Z",
     "start_time": "2024-01-14T20:23:24.531886600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mPxw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43macress\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcress\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[28], line 12\u001B[0m, in \u001B[0;36mPxw\u001B[1;34m(x, w)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mPxw\u001B[39m(x, w):\n\u001B[0;32m     11\u001B[0m     confusion_matrix, characters \u001B[38;5;241m=\u001B[39m find_type_and_letter(x, w)\n\u001B[1;32m---> 12\u001B[0m     soorat \u001B[38;5;241m=\u001B[39m \u001B[43mconfusion_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcharacters\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     13\u001B[0m     makhraj \u001B[38;5;241m=\u001B[39m text_data\u001B[38;5;241m.\u001B[39mcount(characters) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m soorat \u001B[38;5;241m/\u001B[39m makhraj\n",
      "\u001B[1;31mKeyError\u001B[0m: 'a'"
     ]
    }
   ],
   "source": [
    "Pxw('acress', 'cress')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:23:25.531854500Z",
     "start_time": "2024-01-14T20:23:25.375610200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non word\n",
      "candidates :  ['caress', 'acres', 'cress', 'actress', 'across', 'access', \"acre's\", 'a cress', 'acre ss', 'acre-ss', 'acres s']\n",
      "----------\n",
      "word :  caress\n",
      "P(w) is  1.8288709221512644e-06\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m         candidates_dict[candida] \u001B[38;5;241m=\u001B[39m probability_of_w_if_x\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(candidates_dict, key\u001B[38;5;241m=\u001B[39mcandidates_dict\u001B[38;5;241m.\u001B[39mget)\n\u001B[1;32m---> 30\u001B[0m \u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43macress\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[66], line 21\u001B[0m, in \u001B[0;36mapply\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     19\u001B[0m probability_of_w \u001B[38;5;241m=\u001B[39m Pw(candida)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP(w) is \u001B[39m\u001B[38;5;124m\"\u001B[39m, probability_of_w)\n\u001B[1;32m---> 21\u001B[0m probability_of_x_if_w \u001B[38;5;241m=\u001B[39m \u001B[43mPxw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcandida\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP(x|w) is \u001B[39m\u001B[38;5;124m\"\u001B[39m, probability_of_x_if_w)\n\u001B[0;32m     23\u001B[0m probability_of_w_if_x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum([\n\u001B[0;32m     24\u001B[0m     np\u001B[38;5;241m.\u001B[39mlog(probability_of_x_if_w),\n\u001B[0;32m     25\u001B[0m     np\u001B[38;5;241m.\u001B[39mlog(probability_of_w)\n\u001B[0;32m     26\u001B[0m ])\n",
      "Cell \u001B[1;32mIn[23], line 12\u001B[0m, in \u001B[0;36mPxw\u001B[1;34m(x, w)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mPxw\u001B[39m(x, w):\n\u001B[0;32m     11\u001B[0m     confusion_matrix, characters \u001B[38;5;241m=\u001B[39m find_type(x, w)\n\u001B[1;32m---> 12\u001B[0m     soorat \u001B[38;5;241m=\u001B[39m \u001B[43mconfusion_matrix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcharacters\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     13\u001B[0m     makhraj \u001B[38;5;241m=\u001B[39m text_data\u001B[38;5;241m.\u001B[39mcount(characters) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m soorat \u001B[38;5;241m/\u001B[39m makhraj\n",
      "\u001B[1;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def apply(x):\n",
    "    candidates = find_correct_words(x)\n",
    "    # candidates = [\n",
    "    #     \"actress\",\n",
    "    #     \"cress\",\n",
    "    #     \"caress\",\n",
    "    #     \"access\",\n",
    "    # ]\n",
    "    if x in dictionary:\n",
    "        print(\"Real word\")\n",
    "        candidates.insert(0, x)\n",
    "    else:\n",
    "        print(\"Non word\")\n",
    "    print(\"candidates : \", candidates)\n",
    "    candidates_dict = {}\n",
    "    for candida in candidates:\n",
    "        print(\"----------\")\n",
    "        print(\"word : \", candida)\n",
    "        probability_of_w = Pw(candida)\n",
    "        print(\"P(w) is \", probability_of_w)\n",
    "        probability_of_x_if_w = Pxw(x, candida)\n",
    "        print(\"P(x|w) is \", probability_of_x_if_w)\n",
    "        probability_of_w_if_x = np.sum([\n",
    "            np.log(probability_of_x_if_w),\n",
    "            np.log(probability_of_w)\n",
    "        ])\n",
    "        print(\"P(w|x) is \", probability_of_w_if_x)\n",
    "        candidates_dict[candida] = probability_of_w_if_x\n",
    "    return max(candidates_dict, key=candidates_dict.get)\n",
    "apply(\"acress\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:07:26.252021400Z",
     "start_time": "2024-01-14T20:07:26.028302300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
