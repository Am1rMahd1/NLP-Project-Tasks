{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enchant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:52:42.053462100Z",
     "start_time": "2024-01-14T21:52:38.244306400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "file_path = 'Dataset/Spelling Dataset/spell-errors.txt'\n",
    "\n",
    "dictionary_file_path = 'Dataset/Spelling Dataset/test/Dictionary/dictionary.data'\n",
    "\n",
    "del_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/del-confusion.data'\n",
    "ins_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/ins-confusion.data'\n",
    "sub_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/sub-confusion.data'\n",
    "tra_confusion_matrix_file_path = 'Dataset/Spelling Dataset/test/Confusion Matrix/Transposition-confusion.data'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:46.891002300Z",
     "start_time": "2024-01-14T21:53:46.875390500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words in the dictionary:\n",
      "['aa' 'aah' 'aahed' 'aahing' 'aahs' 'aal' 'aalii' 'aaliis' 'aals'\n",
      " 'aardvark']\n"
     ]
    }
   ],
   "source": [
    "def read_dictionary(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        dictionary = np.array([word.strip() for word in file])\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "dictionary = read_dictionary(dictionary_file_path)\n",
    "\n",
    "print(\"First 10 words in the dictionary:\")\n",
    "print(dictionary[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:49.248738700Z",
     "start_time": "2024-01-14T21:53:48.919873900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "        CorrectWord                                 Typos\n0           raining                    [rainning, raning]\n1          writings                           [writtings]\n2     disparagingly                         [disparingly]\n3            yellow                               [yello]\n4              four  [forer, fours, fuore, fore*5, for*4]\n...             ...                                   ...\n7836          jewel                          [jewl, jule]\n7837   commencement                         [commencment]\n7838    suppressing                          [supressing]\n7839         tonner                               [toner]\n7840           sash                                [sach]\n\n[7841 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CorrectWord</th>\n      <th>Typos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>raining</td>\n      <td>[rainning, raning]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>writings</td>\n      <td>[writtings]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>disparagingly</td>\n      <td>[disparingly]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>yellow</td>\n      <td>[yello]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>four</td>\n      <td>[forer, fours, fuore, fore*5, for*4]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7836</th>\n      <td>jewel</td>\n      <td>[jewl, jule]</td>\n    </tr>\n    <tr>\n      <th>7837</th>\n      <td>commencement</td>\n      <td>[commencment]</td>\n    </tr>\n    <tr>\n      <th>7838</th>\n      <td>suppressing</td>\n      <td>[supressing]</td>\n    </tr>\n    <tr>\n      <th>7839</th>\n      <td>tonner</td>\n      <td>[toner]</td>\n    </tr>\n    <tr>\n      <th>7840</th>\n      <td>sash</td>\n      <td>[sach]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7841 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_typo_data(file_path):\n",
    "    typo_data = {'CorrectWord': [], 'Typos': []}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(':')\n",
    "            if len(parts) == 2:\n",
    "                correct_word, typos = parts[0], parts[1].split(',')\n",
    "                typo_data['CorrectWord'].append(correct_word)\n",
    "                typo_data['Typos'].append([typo.strip() for typo in typos])\n",
    "\n",
    "    return pd.DataFrame(typo_data)\n",
    "\n",
    "typo_df = load_typo_data(file_path)\n",
    "typo_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:52.488741600Z",
     "start_time": "2024-01-14T21:53:52.332498500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def read_confusion_matrix(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        confusion_matrix_str = file.read()\n",
    "        confusion_matrix = ast.literal_eval(confusion_matrix_str)\n",
    "\n",
    "    return confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:55.382369900Z",
     "start_time": "2024-01-14T21:53:55.319866400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "del_confusion_matrix = read_confusion_matrix(del_confusion_matrix_file_path)\n",
    "ins_confusion_matrix = read_confusion_matrix(ins_confusion_matrix_file_path)\n",
    "sub_confusion_matrix = read_confusion_matrix(sub_confusion_matrix_file_path)\n",
    "tra_confusion_matrix = read_confusion_matrix(tra_confusion_matrix_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:56.908712100Z",
     "start_time": "2024-01-14T21:53:56.846213500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damerau–Levenshtein distance between 'acress' and 'access': 1\n"
     ]
    }
   ],
   "source": [
    "def damerau_levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the Damerau–Levenshtein distance between two strings.\n",
    "    \"\"\"\n",
    "    len_s1 = len(s1)\n",
    "    len_s2 = len(s2)\n",
    "    d = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]\n",
    "\n",
    "    for i in range(len_s1 + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len_s2 + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len_s1 + 1):\n",
    "        for j in range(1, len_s2 + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            d[i][j] = min(\n",
    "                d[i - 1][j] + 1,  # deletion\n",
    "                d[i][j - 1] + 1,  # insertion\n",
    "                d[i - 1][j - 1] + cost,  # substitution\n",
    "            )\n",
    "            if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)  # transposition\n",
    "\n",
    "    return d[len_s1][len_s2]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "s1 = \"acress\"\n",
    "s2 = \"access\"\n",
    "\n",
    "distance = damerau_levenshtein_distance(s1, s2)\n",
    "print(f\"Damerau–Levenshtein distance between '{s1}' and '{s2}': {distance}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:53:57.578992400Z",
     "start_time": "2024-01-14T21:53:57.485239500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "['problems']"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_correct_words(typo):\n",
    "    # correct_words = typo_df.loc[typo_df['Typos'].apply(lambda x: typo in x), 'CorrectWord'].tolist()\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    correct_words = d.suggest(typo)\n",
    "    correct_words2 = []\n",
    "    for candida in correct_words:\n",
    "        if candida.isalpha() and damerau_levenshtein_distance(typo, candida) == 1:\n",
    "            correct_words2.append(candida)\n",
    "    return correct_words2\n",
    "\n",
    "typo = 'problem'\n",
    "correct_words = find_correct_words(typo)\n",
    "correct_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T22:13:07.843766200Z",
     "start_time": "2024-01-14T22:13:07.591540Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "with open('Dataset/Spelling Dataset/test/Dictionary/Dataset.data', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Split the text into sentences or paragraphs based on your dataset structure\n",
    "sentences = text_data.split('<s>')\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "tokenized_sentences = np.concatenate(tokenized_sentences, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:54:06.006033500Z",
     "start_time": "2024-01-14T21:54:00.562334100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "             word   count\n72392         the  282764\n5941          and  155164\n51147          of  151505\n73254          to  115135\n37328          in   87003\n...           ...     ...\n39687  jaculation       1\n39690       jadau       1\n39694       jadon       1\n39695      jaeger       1\n81342        zzzz       1\n\n[81343 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>72392</th>\n      <td>the</td>\n      <td>282764</td>\n    </tr>\n    <tr>\n      <th>5941</th>\n      <td>and</td>\n      <td>155164</td>\n    </tr>\n    <tr>\n      <th>51147</th>\n      <td>of</td>\n      <td>151505</td>\n    </tr>\n    <tr>\n      <th>73254</th>\n      <td>to</td>\n      <td>115135</td>\n    </tr>\n    <tr>\n      <th>37328</th>\n      <td>in</td>\n      <td>87003</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39687</th>\n      <td>jaculation</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39690</th>\n      <td>jadau</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39694</th>\n      <td>jadon</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39695</th>\n      <td>jaeger</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>81342</th>\n      <td>zzzz</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>81343 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, counts = np.unique(tokenized_sentences, return_counts=True)\n",
    "dataset = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'count': counts\n",
    "}).sort_values(['count'], ascending=False)\n",
    "# dataset.map(lambda x : apply())\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T21:54:12.413616500Z",
     "start_time": "2024-01-14T21:54:07.434321400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deletion\n"
     ]
    },
    {
     "data": {
      "text/plain": "({'gw': 1,\n  'gv': 0,\n  'gu': 3,\n  'gt': 2,\n  'gs': 69,\n  'gr': 5,\n  'gq': 1,\n  'gp': 0,\n  '#y': 1,\n  'gz': 0,\n  'gy': 0,\n  'gx': 0,\n  'gg': 5,\n  'gf': 1,\n  'ge': 5,\n  'gd': 0,\n  'gc': 0,\n  'gb': 0,\n  'ga': 8,\n  'go': 1,\n  'gn': 1,\n  'gm': 0,\n  'gl': 2,\n  'gk': 0,\n  'gj': 0,\n  'gi': 8,\n  'gh': 12,\n  'tz': 0,\n  'tx': 0,\n  'ty': 6,\n  'tv': 0,\n  'tw': 5,\n  'tt': 183,\n  'tu': 11,\n  'tr': 54,\n  'ts': 264,\n  'tp': 1,\n  'tq': 0,\n  'tn': 1,\n  'to': 23,\n  'tl': 6,\n  'tm': 3,\n  'tj': 1,\n  'tk': 0,\n  'th': 24,\n  'ti': 59,\n  'tf': 1,\n  'tg': 10,\n  'td': 3,\n  'te': 65,\n  'tb': 0,\n  'tc': 0,\n  'ta': 39,\n  'vu': 1,\n  'zl': 0,\n  'zm': 0,\n  'zn': 0,\n  'zo': 0,\n  'zh': 0,\n  'zi': 6,\n  'zj': 0,\n  'zk': 0,\n  'zd': 0,\n  'ze': 5,\n  'zf': 1,\n  'zg': 0,\n  'za': 2,\n  'zb': 0,\n  'zc': 0,\n  'zx': 0,\n  'zy': 2,\n  'zz': 4,\n  'zt': 0,\n  'zu': 0,\n  'zv': 0,\n  'zw': 0,\n  'zp': 0,\n  'zq': 0,\n  'zr': 0,\n  'zs': 0,\n  '#s': 23,\n  'wl': 1,\n  '#q': 0,\n  'va': 0,\n  '#c': 9,\n  'vc': 0,\n  'wk': 1,\n  '#p': 10,\n  'vh': 0,\n  'wj': 0,\n  'vi': 10,\n  'vj': 0,\n  'vk': 0,\n  'vl': 1,\n  'vm': 0,\n  'wi': 1,\n  '#v': 1,\n  'vn': 1,\n  'vo': 0,\n  'me': 17,\n  'md': 0,\n  'mg': 0,\n  'mf': 0,\n  'ma': 11,\n  'mc': 1,\n  'mb': 1,\n  'mm': 102,\n  'ml': 0,\n  'mo': 7,\n  'mn': 44,\n  'mi': 6,\n  'mh': 1,\n  'mk': 1,\n  'mj': 0,\n  'mu': 2,\n  'mt': 1,\n  'mw': 1,\n  'mv': 0,\n  'mq': 0,\n  'mp': 2,\n  'ms': 47,\n  'mr': 0,\n  'vt': 0,\n  'my': 0,\n  'mx': 0,\n  'mz': 0,\n  'vv': 5,\n  'vw': 1,\n  '#t': 2,\n  'vx': 0,\n  'vz': 0,\n  '#b': 8,\n  'fp': 0,\n  'fq': 0,\n  'fr': 5,\n  'fs': 23,\n  'ft': 0,\n  'fu': 1,\n  'fv': 0,\n  'fw': 0,\n  'fx': 0,\n  'fy': 1,\n  'fz': 0,\n  'fa': 1,\n  'fb': 0,\n  'fc': 0,\n  'fd': 0,\n  'fe': 2,\n  'ff': 27,\n  'fg': 1,\n  'fh': 0,\n  'fi': 12,\n  'fj': 0,\n  'fk': 0,\n  'fl': 10,\n  'fm': 0,\n  'fn': 0,\n  'fo': 0,\n  'sz': 0,\n  'sy': 7,\n  'sx': 0,\n  'ss': 205,\n  'sr': 1,\n  'sq': 0,\n  'sp': 1,\n  'sw': 1,\n  'sv': 0,\n  'su': 7,\n  'st': 49,\n  'sk': 2,\n  'sj': 0,\n  'si': 101,\n  'sh': 50,\n  'so': 3,\n  'sn': 7,\n  'sm': 10,\n  'sl': 2,\n  'sc': 7,\n  'sb': 1,\n  'sa': 13,\n  'sg': 1,\n  'sf': 0,\n  'se': 41,\n  'sd': 20,\n  'lf': 0,\n  'lg': 0,\n  'ld': 1,\n  'le': 38,\n  'lb': 1,\n  'lc': 0,\n  'la': 3,\n  'ln': 0,\n  'lo': 7,\n  'll': 128,\n  'lm': 1,\n  'lj': 0,\n  'lk': 2,\n  'lh': 0,\n  'li': 79,\n  'lv': 1,\n  'lw': 0,\n  'lt': 7,\n  'lu': 3,\n  'lr': 0,\n  'ls': 97,\n  'lp': 0,\n  'lq': 0,\n  'lz': 0,\n  'lx': 0,\n  'ly': 2,\n  'wq': 1,\n  'yh': 0,\n  'yk': 0,\n  'yj': 0,\n  'ym': 1,\n  'yl': 1,\n  'yo': 0,\n  'yn': 6,\n  'ya': 5,\n  'yc': 2,\n  'yb': 1,\n  'ye': 3,\n  'yd': 0,\n  'yg': 0,\n  'yf': 0,\n  'yy': 2,\n  'yx': 0,\n  'yz': 0,\n  'yq': 0,\n  'yp': 0,\n  'ys': 33,\n  'yr': 1,\n  'yu': 13,\n  'yt': 1,\n  'yw': 1,\n  'yv': 0,\n  'em': 6,\n  'el': 4,\n  'eo': 5,\n  'en': 27,\n  'ei': 4,\n  'eh': 1,\n  'ek': 3,\n  'ej': 0,\n  'ee': 147,\n  'ed': 76,\n  'eg': 0,\n  'ef': 2,\n  'ea': 39,\n  'ec': 8,\n  'eb': 2,\n  'ey': 8,\n  'ex': 2,\n  '#g': 14,\n  'ez': 0,\n  'eu': 4,\n  'et': 6,\n  'ew': 10,\n  'ev': 1,\n  'eq': 0,\n  'ep': 1,\n  'es': 417,\n  'er': 83,\n  'rt': 29,\n  'ru': 7,\n  'rv': 0,\n  'rw': 1,\n  'rp': 0,\n  'rq': 0,\n  'rr': 132,\n  'rs': 273,\n  'rx': 0,\n  'ry': 10,\n  'rz': 0,\n  'rd': 0,\n  're': 89,\n  'rf': 1,\n  'rg': 1,\n  'ra': 15,\n  'rb': 2,\n  'rc': 1,\n  'rl': 5,\n  'rm': 9,\n  'rn': 7,\n  'ro': 10,\n  'rh': 2,\n  'ri': 64,\n  'rj': 0,\n  'rk': 0,\n  'xj': 0,\n  'xk': 0,\n  'xh': 6,\n  'xi': 1,\n  'xn': 0,\n  'xo': 3,\n  'xl': 0,\n  'xm': 1,\n  'xb': 0,\n  'xc': 18,\n  'xa': 0,\n  'xf': 0,\n  'xg': 0,\n  'xd': 0,\n  'xe': 1,\n  'xz': 0,\n  'xx': 1,\n  'xy': 0,\n  'xr': 0,\n  'xs': 2,\n  'xp': 0,\n  'xq': 0,\n  'xv': 0,\n  'xw': 0,\n  'xt': 0,\n  'xu': 0,\n  'wy': 0,\n  'wx': 0,\n  '#d': 8,\n  'kc': 0,\n  'kb': 4,\n  'ka': 2,\n  'kg': 0,\n  'kf': 0,\n  'ke': 9,\n  'kd': 1,\n  'kk': 1,\n  'kj': 0,\n  'ki': 1,\n  'kh': 1,\n  'ko': 2,\n  'kn': 0,\n  'km': 0,\n  'kl': 1,\n  'ks': 95,\n  'kr': 0,\n  'kq': 0,\n  'kp': 1,\n  'kw': 0,\n  'kv': 0,\n  'ku': 1,\n  'kt': 0,\n  'kz': 0,\n  'ky': 4,\n  'kx': 0,\n  'dn': 9,\n  'do': 13,\n  'dl': 6,\n  'dm': 1,\n  'dj': 0,\n  'dk': 0,\n  'dh': 0,\n  'di': 9,\n  'df': 2,\n  'dg': 0,\n  'dd': 17,\n  'de': 14,\n  'db': 0,\n  'dc': 3,\n  'da': 18,\n  'dz': 0,\n  'dx': 0,\n  'dy': 5,\n  'dv': 0,\n  'dw': 0,\n  'dt': 0,\n  'du': 0,\n  'dr': 6,\n  'ds': 119,\n  'dp': 0,\n  'dq': 0,\n  'qq': 0,\n  'qp': 0,\n  'qs': 0,\n  'qr': 0,\n  'qu': 1,\n  'qt': 0,\n  'qw': 0,\n  'qv': 0,\n  'qy': 0,\n  'qx': 0,\n  'qz': 0,\n  'qa': 0,\n  'qc': 0,\n  'qb': 0,\n  'qe': 0,\n  'qd': 0,\n  'qg': 0,\n  'qf': 0,\n  'qi': 1,\n  'qh': 0,\n  'qk': 0,\n  'qj': 0,\n  'qm': 0,\n  'ql': 0,\n  'qo': 0,\n  'qn': 0,\n  '#k': 17,\n  '#j': 1,\n  '#e': 26,\n  '#i': 5,\n  '#h': 3,\n  'wc': 0,\n  'wb': 0,\n  'wa': 0,\n  'wo': 0,\n  'wn': 2,\n  'wm': 0,\n  'wg': 0,\n  'wf': 0,\n  'we': 10,\n  'wd': 1,\n  'jx': 0,\n  'jy': 0,\n  'jz': 0,\n  '#l': 5,\n  'jt': 0,\n  'ju': 1,\n  'jv': 0,\n  'jw': 0,\n  'jp': 0,\n  'jq': 0,\n  'jr': 0,\n  'js': 0,\n  'jl': 0,\n  'jm': 0,\n  'jn': 0,\n  'jo': 0,\n  'jh': 0,\n  'ji': 0,\n  'jj': 0,\n  'jk': 0,\n  'jd': 0,\n  'je': 0,\n  'jf': 0,\n  'jg': 0,\n  '#w': 2,\n  'ja': 0,\n  'jb': 0,\n  'jc': 0,\n  'ww': 4,\n  'wv': 0,\n  'wu': 2,\n  'wt': 0,\n  'ws': 8,\n  'wr': 1,\n  'ck': 3,\n  'cj': 0,\n  'ci': 50,\n  'ch': 18,\n  'co': 7,\n  'cn': 1,\n  'cm': 1,\n  'cl': 1,\n  'cc': 54,\n  'cb': 0,\n  'ca': 19,\n  'wp': 0,\n  'cg': 0,\n  'cf': 0,\n  'ce': 13,\n  'cd': 1,\n  'cz': 0,\n  'cy': 0,\n  'cx': 1,\n  '#r': 6,\n  'cs': 25,\n  'cr': 7,\n  'cq': 0,\n  'cp': 1,\n  'cw': 0,\n  'cv': 4,\n  'cu': 8,\n  'ct': 7,\n  'pr': 29,\n  'ps': 52,\n  'pp': 70,\n  'pq': 0,\n  'pv': 1,\n  'pw': 1,\n  'pt': 9,\n  'pu': 1,\n  'pz': 0,\n  'px': 0,\n  'py': 0,\n  'wz': 0,\n  'pb': 0,\n  'pc': 1,\n  'pa': 23,\n  'pf': 0,\n  'pg': 0,\n  'pd': 1,\n  'pe': 10,\n  'pj': 0,\n  'pk': 0,\n  'ph': 20,\n  'pi': 3,\n  'pn': 0,\n  'po': 26,\n  'pl': 2,\n  'pm': 0,\n  'iy': 0,\n  'ix': 1,\n  'vb': 2,\n  'iz': 1,\n  'vd': 0,\n  've': 36,\n  'vf': 0,\n  'vg': 0,\n  'iq': 0,\n  'ip': 1,\n  'is': 30,\n  'ir': 9,\n  'iu': 11,\n  'it': 29,\n  'iw': 0,\n  'iv': 0,\n  'ii': 69,\n  'ih': 1,\n  'ik': 1,\n  'ij': 2,\n  'im': 11,\n  'il': 17,\n  'io': 27,\n  'in': 33,\n  'ia': 10,\n  'vy': 0,\n  'ic': 13,\n  'ib': 3,\n  'ie': 25,\n  'id': 13,\n  'ig': 1,\n  'if': 0,\n  '#x': 1,\n  'wh': 1,\n  'yi': 2,\n  '#u': 11,\n  'vr': 0,\n  '#f': 11,\n  '#o': 2,\n  '#n': 2,\n  '#m': 6,\n  'vs': 0,\n  'bd': 0,\n  'be': 7,\n  'bf': 0,\n  'bg': 1,\n  'ba': 3,\n  'bb': 11,\n  'bc': 0,\n  'bl': 15,\n  'bm': 0,\n  'bn': 1,\n  'bo': 1,\n  'bh': 0,\n  'bi': 50,\n  'bj': 0,\n  'bk': 0,\n  'bt': 0,\n  'bu': 0,\n  'bv': 3,\n  'bw': 0,\n  'bp': 0,\n  'bq': 0,\n  'br': 5,\n  'bs': 16,\n  'bx': 0,\n  'by': 0,\n  'bz': 0,\n  'oo': 64,\n  'on': 13,\n  'om': 3,\n  'ol': 6,\n  'ok': 0,\n  'oj': 1,\n  'oi': 28,\n  'oh': 0,\n  'og': 1,\n  'of': 2,\n  'oe': 7,\n  'od': 3,\n  'oc': 1,\n  'ob': 1,\n  'oa': 14,\n  'oz': 1,\n  'oy': 1,\n  'ox': 0,\n  'ow': 0,\n  'ov': 1,\n  'ou': 19,\n  'ot': 4,\n  'os': 59,\n  'or': 16,\n  'oq': 0,\n  'op': 30,\n  '#a': 46,\n  'hz': 0,\n  'hx': 0,\n  'hy': 3,\n  'hr': 16,\n  'hs': 24,\n  'hp': 0,\n  'hq': 0,\n  'hv': 0,\n  'hw': 5,\n  'ht': 22,\n  'hu': 1,\n  'hj': 2,\n  'hk': 0,\n  'hh': 18,\n  'hi': 17,\n  'hn': 1,\n  'ho': 4,\n  'hl': 1,\n  'hm': 0,\n  'hb': 1,\n  'hc': 0,\n  'ha': 4,\n  'hf': 0,\n  'hg': 10,\n  'hd': 1,\n  'he': 24,\n  'uy': 3,\n  'ux': 2,\n  'uz': 0,\n  'uu': 26,\n  'ut': 27,\n  'uw': 0,\n  'uv': 0,\n  'uq': 0,\n  'up': 3,\n  'us': 19,\n  'ur': 49,\n  'um': 3,\n  'ul': 3,\n  'uo': 1,\n  'un': 9,\n  'ui': 24,\n  'uh': 1,\n  'uk': 1,\n  'uj': 1,\n  'ue': 9,\n  'ud': 0,\n  'ug': 0,\n  'uf': 0,\n  'ua': 15,\n  'uc': 3,\n  'ub': 0,\n  'aa': 15,\n  'ac': 14,\n  'ab': 1,\n  'ae': 10,\n  'ad': 7,\n  'ag': 1,\n  'af': 0,\n  'ai': 33,\n  'ah': 1,\n  'ak': 4,\n  'aj': 1,\n  'am': 2,\n  'al': 31,\n  'ao': 12,\n  'an': 39,\n  'aq': 3,\n  'ap': 4,\n  'as': 134,\n  'ar': 28,\n  'au': 28,\n  'at': 7,\n  'aw': 1,\n  'av': 0,\n  'ay': 4,\n  'ax': 1,\n  'az': 1,\n  'nh': 0,\n  'ni': 34,\n  'nj': 0,\n  'nk': 1,\n  'nl': 1,\n  'nm': 26,\n  'nn': 99,\n  'no': 12,\n  'na': 15,\n  'nb': 5,\n  'nc': 7,\n  'nd': 13,\n  'ne': 52,\n  'nf': 4,\n  'ng': 17,\n  'nx': 0,\n  'ny': 1,\n  'nz': 0,\n  'np': 0,\n  'nq': 0,\n  'nr': 2,\n  'ns': 156,\n  'nt': 53,\n  'nu': 1,\n  'nv': 1,\n  'nw': 0,\n  'vp': 1,\n  '#z': 2,\n  'vq': 0},\n 'de')"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_type_and_letter(x, w):\n",
    "    if len(x) > len(w):\n",
    "        for i in range(len(x)):\n",
    "            if i == len(w) - 1 or w[i] != x[i]:\n",
    "                print(\"insertion\")\n",
    "                first_char = x[i-1] if i!=0 else x[i+1]\n",
    "                return ins_confusion_matrix, first_char+x[i]\n",
    "            else:\n",
    "                continue\n",
    "    elif len(x) < len(w):\n",
    "        for i in range(len(w)):\n",
    "            if i == len(x) - 1 or w[i] != x[i]:\n",
    "                print(\"deletion\")\n",
    "                first_char = x[i-1] if i!=0 else x[i+1]\n",
    "                return ins_confusion_matrix, first_char+x[i]\n",
    "            else:\n",
    "                continue\n",
    "    else: #transposition or substitution\n",
    "        for i in range(len(x)):\n",
    "            if w[i] == x[i]:\n",
    "                continue\n",
    "            else:\n",
    "                if i == len(x)-1:\n",
    "                    print(\"substitution\")\n",
    "                    return sub_confusion_matrix, w[i]+x[i]\n",
    "                else:\n",
    "                    if w[i+1] == x[i+1]:\n",
    "                        print(\"substitution\")\n",
    "                        return sub_confusion_matrix, w[i]+x[i]\n",
    "                    elif w[i] == x[i+1]:\n",
    "                        print(\"transposition\")\n",
    "                        return tra_confusion_matrix, w[i]+w[i+1]\n",
    "                    else:\n",
    "                        return \"?\"\n",
    "candidates = ['caress', 'acres', 'cress', 'actress', 'across', 'access', \"acre's\", 'a cress', 'acre ss', 'acre-ss', 'acres s']\n",
    "find_type_and_letter(\"contende\", \"contender\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T22:18:07.014425700Z",
     "start_time": "2024-01-14T22:18:06.889438400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def Pw(w):\n",
    "    if not dataset.loc[dataset['word'] == w].empty :\n",
    "        frequency_of_w = dataset.loc[dataset['word'] == w].iloc[0]['count']\n",
    "    else:\n",
    "        frequency_of_w = 0\n",
    "    count_of_all_tokens = dataset['count'].sum()\n",
    "    size_of_vocab = len(dataset['word'])\n",
    "    return (frequency_of_w + 1) / (count_of_all_tokens + size_of_vocab)\n",
    "\n",
    "def Pxw(x, w):\n",
    "    confusion_matrix, characters = find_type_and_letter(x, w)\n",
    "    soorat = confusion_matrix[characters] + 1\n",
    "    makhraj = text_data.count(characters) + 1\n",
    "    return soorat / makhraj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T22:20:27.821288100Z",
     "start_time": "2024-01-14T22:20:27.774413700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deletion\n",
      "de\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.00013100551096516128"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pxw('contende', 'contender')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T22:18:13.185776100Z",
     "start_time": "2024-01-14T22:18:13.107652800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non word\n",
      "candidates :  ['ecstasy', 'eustacy']\n",
      "----------\n",
      "word :  ecstasy\n",
      "P(w) is  4.876989125736705e-06\n",
      "substitution\n",
      "P(x|w) is  0.0016937874296775754\n",
      "P(w|x) is  -18.611770685820016\n",
      "----------\n",
      "word :  eustacy\n",
      "P(w) is  2.0320788023902935e-07\n",
      "substitution\n",
      "P(x|w) is  3.997281848343127e-05\n",
      "P(w|x) is  -25.536347214045247\n"
     ]
    },
    {
     "data": {
      "text/plain": "'ecstasy'"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply(x):\n",
    "    candidates = find_correct_words(x)\n",
    "    # candidates = [\n",
    "    #     \"actress\",\n",
    "    #     \"cress\",\n",
    "    #     \"caress\",\n",
    "    #     \"access\",\n",
    "    # ]\n",
    "    if x in dictionary:\n",
    "        print(\"Real word\")\n",
    "        candidates.insert(0, x)\n",
    "    else:\n",
    "        print(\"Non word\")\n",
    "    print(\"candidates : \", candidates)\n",
    "    candidates_dict = {}\n",
    "    for candida in candidates:\n",
    "        print(\"----------\")\n",
    "        print(\"word : \", candida)\n",
    "        probability_of_w = Pw(candida)\n",
    "        print(\"P(w) is \", probability_of_w)\n",
    "        probability_of_x_if_w = Pxw(x, candida)\n",
    "        print(\"P(x|w) is \", probability_of_x_if_w)\n",
    "        probability_of_w_if_x = np.sum([\n",
    "            np.log(probability_of_x_if_w),\n",
    "            np.log(probability_of_w)\n",
    "        ])\n",
    "        print(\"P(w|x) is \", probability_of_w_if_x)\n",
    "        candidates_dict[candida] = probability_of_w_if_x\n",
    "    return max(candidates_dict, key=candidates_dict.get)\n",
    "apply(\"ecstacy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T22:20:31.104028300Z",
     "start_time": "2024-01-14T22:20:30.701660100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
